_target_: rnn.agents.sarnn_agent.SARNN_Agent
_recursive_: false

optimization:
  _target_: torch.optim.AdamW
  lr: 1.0e-4
  betas: [0.95, 0.999]
  eps: 1.0e-8
  weight_decay: 1.0e-6

# lr_scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   #step_size: 100
#   # gamma: 0.99
#   T_max: 100000
#   eta_min: 1e-6

model:
  _target_: rnn.models.sarnn.SARNN
  _recursive_: false

  rec_dim: 128
  obs_dim: ${obs_dim}
  action_dim: ${action_dim}
  k_dim: 5
  temperature: 1e-4
  heatmap_size: 0.1
  kernel_size: 3
  im_size: [96, 96]

trainset:
  _target_: rnn.dataset_wrappers.sorting_dataset.Sorting_Img_Dataset_Wrapper
  dataset: ${trainset}
  skip: 3

valset:
  _target_: rnn.dataset_wrappers.sorting_dataset.Sorting_Img_Dataset_Wrapper
  dataset: ${valset}
  skip: 3

train_batch_size: ${train_batch_size}
val_batch_size: ${val_batch_size}
num_workers: ${num_workers}
epoch: ${epoch}
device: ${device}
scale_data: ${scale_data}
eval_every_n_epochs: ${eval_every_n_epochs}

# action_seq_size: ${window_size}
# obs_size: 1

# goal_conditioned: false
# decay: 0
# goal_window_size: 1
# window_size: ${window_size}
# patience: 80 # interval for early stopping during epoch training
loss_weights: {"image": 0.1, "action": 1.0, "attention": 0.1}